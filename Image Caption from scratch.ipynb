{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os,sys\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_path_train = r\"C:/Users/Kerem/Desktop/nn_final/ims/train/*.jpg\" \n",
    "# im_path_test = r\"C:/Users/Kerem/Desktop/nn_final/ims/test/*.jpg\"\n",
    "im_path_train = r\"/media/seibutsugakun/6C1C500A1C4FCDB0/Users/Kerem/Desktop/nn_final/ims/train/*.jpg\"\n",
    "im_path_test = r\"/media/seibutsugakun/6C1C500A1C4FCDB0/Users/Kerem/Desktop/nn_final/ims/test/*.jpg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below code will show all pictures in the given path. In this case , it will search for those ending with .jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = r\"C:/Users/Kerem/Desktop/nn_final/ims/*.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for filename in glob.glob(my_path):\n",
    "    im=Image.open(filename)\n",
    "    im.show()\n",
    "    image_list.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in image_list:\n",
    "    a = np.asarray(i)\n",
    "    print(\"image size = \",a.shape)\n",
    "    plt.imshow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get minibatch train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (120,120) into shape (120,120,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d850b6552cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (120,120) into shape (120,120,3)"
     ]
    }
   ],
   "source": [
    "train_image_list = []\n",
    "minibatch_size = 200\n",
    "no_of_images = len(glob.glob(im_path_train)) #75613\n",
    "\n",
    "current_batch_names = glob.glob(im_path_train)[0:minibatch_size]\n",
    "current_batch = np.zeros((minibatch_size,120,120,3))\n",
    "\n",
    "for idx,im in enumerate(current_batch_names):\n",
    "    current_batch[idx,:,:,:] = np.asarray(Image.open(im))\n",
    "\n",
    "current_batch.shape\n",
    "\n",
    "# for filename in glob.glob(im_path_train):\n",
    "#     im=Image.open(filename)\n",
    "#     train_image_list.append(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builiding Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_(img, conv_filter):\n",
    "    # summary: gets called on each filter and each channel (RGB)\n",
    "    filter_size = conv_filter.shape[1] #i.e. 3\n",
    "    result = np.zeros((img.shape[0:2]))\n",
    "    #Looping through the image to apply the convolution operation.\n",
    "    for r in np.uint16(np.arange(filter_size/2.0, \n",
    "                          img.shape[0]-filter_size/2.0+1)):\n",
    "        for c in np.uint16(np.arange(filter_size/2.0, \n",
    "                                           img.shape[1]-filter_size/2.0+1)):\n",
    "            \"\"\"\n",
    "            Getting the current region to get multiplied with the filter.\n",
    "            How to loop through the image and get the region based on \n",
    "            the image and filter sizes is the most tricky part of convolution.\n",
    "            \"\"\"\n",
    "            curr_region = img[r-np.uint16(np.floor(filter_size/2.0)):r+np.uint16(np.ceil(filter_size/2.0)), \n",
    "                              c-np.uint16(np.floor(filter_size/2.0)):c+np.uint16(np.ceil(filter_size/2.0))]\n",
    "            #Element-wise multipliplication between the current region and the filter.\n",
    "            curr_result = curr_region * conv_filter\n",
    "            conv_sum = np.sum(curr_result) #Summing the result of multiplication.\n",
    "            result[r, c] = conv_sum #Saving the summation in the convolution layer feature map.\n",
    "            \n",
    "    #Clipping the outliers of the result matrix.\n",
    "    final_result = result[np.uint16(filter_size/2.0):result.shape[0]-np.uint16(filter_size/2.0), \n",
    "                          np.uint16(filter_size/2.0):result.shape[1]-np.uint16(filter_size/2.0)]\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(img, conv_filter):\n",
    "    if len(img.shape) > 2 or len(conv_filter.shape) > 3: # Check if number of image channels matches the filter depth.\n",
    "        if img.shape[-1] != conv_filter.shape[-1]:\n",
    "            print(\"Error: Number of channels in both image and filter must match.\")\n",
    "            sys.exit()\n",
    "    if conv_filter.shape[1] != conv_filter.shape[2]: # Check if filter dimensions are equal.\n",
    "        print('Error: Filter must be a square matrix. I.e. number of rows and columns must match.')\n",
    "        sys.exit()\n",
    "    if conv_filter.shape[1]%2==0: # Check if filter diemnsions are odd.\n",
    "        print('Error: Filter must have an odd size. I.e. number of rows and columns must be odd.')\n",
    "        sys.exit()\n",
    "\n",
    "    # An empty feature map to hold the output of convolving the filter(s) with the image.\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filter.shape[1]+1, \n",
    "                                img.shape[1]-conv_filter.shape[1]+1, \n",
    "                                conv_filter.shape[0]))\n",
    "\n",
    "    # Convolving the image by the filter(s).\n",
    "    for filter_num in range(conv_filter.shape[0]):\n",
    "        print(\"Filter \", filter_num + 1)\n",
    "        curr_filter = conv_filter[filter_num, :] # getting a filter from the bank.\n",
    "        \"\"\" \n",
    "        Checking if there are mutliple channels for the single filter.\n",
    "        If so, then each channel will convolve the image.\n",
    "        The result of all convolutions are summed to return a single feature map.\n",
    "        \"\"\"\n",
    "# #         print(\"current filter shape is \",curr_filter.shape)\n",
    "#         if img.shape[-1] >= 2:\n",
    "#             for depth in range(curr_filter.shape[-1]):\n",
    "#                 conv_map = conv_(img[:, :, 0], curr_filter(:,:,:,depth)) # Array holding the sum of all feature maps.\n",
    "#                 for ch_num in range(1,img.shape[-1]): # Convolving each channel with the image and summing the results.\n",
    "#                     conv_map = conv_map + conv_(img[:, :, ch_num], \n",
    "#                                       curr_filter)\n",
    "                    \n",
    "        if len(curr_filter.shape) > 2:  \n",
    "            conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # Array holding the sum of all feature maps.  \n",
    "            for ch_num in range(1, curr_filter.shape[-1]): # Convolving each channel with the image and summing the results.  \n",
    "                 conv_map = conv_map + conv_(img[:, :, ch_num],   \n",
    "                                   curr_filter[:, :, ch_num])  \n",
    "                       \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        else: # There is just a single channel in the filter.\n",
    "            conv_map = conv_(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map # Holding feature map with the current filter.\n",
    "    return feature_maps # Returning all feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(feature_map, size=2, stride=2):\n",
    "    #Preparing the output of the pooling operation.\n",
    "    pool_out = np.zeros((np.uint16((feature_map.shape[0]-size+1)/stride+1),\n",
    "                            np.uint16((feature_map.shape[1]-size+1)/stride+1),\n",
    "                            feature_map.shape[-1]))\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        r2 = 0\n",
    "        for r in np.arange(0,feature_map.shape[0]-size+1, stride):\n",
    "            c2 = 0\n",
    "            for c in np.arange(0, feature_map.shape[1]-size+1, stride):\n",
    "                pool_out[r2, c2, map_num] = np.max([feature_map[r:r+size,  c:c+size, map_num]])\n",
    "                c2 = c2 + 1\n",
    "            r2 = r2 +1\n",
    "    return pool_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(feature_map):\n",
    "    #Preparing the output of the ReLU activation function.\n",
    "    relu_out = np.zeros(feature_map.shape)\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        for r in np.arange(0,feature_map.shape[0]):\n",
    "            for c in np.arange(0, feature_map.shape[1]):\n",
    "                relu_out[r, c, map_num] = np.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_batchnorm():\n",
    "        \n",
    "    #for CNN:\n",
    "    obj.previousInput = input;\n",
    "    [H,W,C,N] = size(input);\n",
    "\n",
    "    # Permute the dimensions to the following format \n",
    "    # (cols, channel, rows, batch)    \n",
    "    # Python tensor format:\n",
    "    # (batch(0), channel(1), rows(2), cols(3))\n",
    "    xT = x.transpose((0,2,3,1))\n",
    "    \n",
    "    #Flat the input (On python the reshape is row-major)           \n",
    "    \n",
    "#     inputFlat = reshape_row_major(inputTransposed,[(numel(inputTransposed) / C),C]);\n",
    "    inputFlat = flatten(xT)\n",
    "    \n",
    "    #Call the forward propagation of normal batchnorm\n",
    "    activations = obj.normalBatchNorm.ForwardPropagation(inputFlat, weights, bias);\n",
    "    \n",
    "    #Reshape/transpose back the signal, on python was (N,H,W,C)\n",
    "    activations_reshape = activations.transpose((N,H,W,C))\n",
    "    \n",
    "    #On python was transpose(0,3,1,2)\n",
    "    activations = np.transpose(activations_reshape,[3 1 2 4]);\n",
    "\n",
    "    #Store stuff for backpropagation\n",
    "    obj.activations = activations;\n",
    "    obj.weights = weights;\n",
    "    obj.biases = bias;\n",
    "    \n",
    "    mode = bn_param['mode']\n",
    "    epsilon=1e-6\n",
    "    momentum = bn_param.get('momentum',0.9)\n",
    "    \n",
    "    N,D = x.shape\n",
    "    running_mean = bn_param.get('running_mean',np.zeros(D,dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var',np.zeros(D,dtype=x.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm(x,gamma,beta,bn_param):\n",
    "    #comes before relu // different for FC and CNN (needs reshape)\n",
    "\n",
    "    mode = bn_param['mode']\n",
    "    epsilon = bn_param.get('eps',1e-5)\n",
    "    momentum = bn_param.get('momentum',0.9)\n",
    "    \n",
    "    N,D = x.shape\n",
    "    running_mean = bn_param.get('running_mean',np.zeros(D,dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var',np.zeros(D,dtype=x.dtype))\n",
    "    \n",
    "    out,cache = None,None\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        \n",
    "        mu = 1/ float(N) * np.sum(x, axis=0)\n",
    "        xmu = x - mu\n",
    "        carre = xmu**2\n",
    "        var = 1/ float(N) * np.sum(carre,axis=0)\n",
    "        sqrtvar = np.sqrt(var + epsilon)\n",
    "        invvar = 1. / sqrtvar\n",
    "        va2 = xmu * invvar\n",
    "        va3 = gamma * va2\n",
    "        out = va3 + beta\n",
    "        \n",
    "        running_mean = momentum * running_mean +(1.0 - momentum) * mu\n",
    "        running_var = momentum * running_var + (1.0 - momentum) * var\n",
    "        #store values\n",
    "        cache = (mu,xmu,carre,var,sqrtvar,invvar,va2,va3,gamma,beta,x,bn_param)\n",
    "    \n",
    "    elif mode == \"test\":\n",
    "        running_mean = bn_param['running_mean']\n",
    "        running_var = bn_param['running_var']\n",
    "        xbar = (x-running_mean) /np.sqrt(running_var+epsilon)\n",
    "        out = gamma * xbar + beta\n",
    "        cache = (x,xbar,gamma,beta,epsilon)\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode')\n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "    \n",
    "    return out,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_filter = np.zeros((2,3,3)) # 2 filters with 3x3 2D-convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set first filters to look for edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_filter[0, :, :] = np.array([[[-1, 0, 1],   \n",
    "                                     [-1, 0, 1],   \n",
    "                                     [-1, 0, 1]]])  \n",
    "l1_filter[1, :, :] = np.array([[[1,   1,  1],   \n",
    "                                     [0,   0,  0],   \n",
    "                                     [-1, -1, -1]]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now apply multiple convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_cnn_layers(img,l1_filter,gamma=0,beta=0.01,is_training=True):    \n",
    "    print(\"Input shape:\",img.shape)\n",
    "    \n",
    "    num_of_filters = 2\n",
    "    kernel_size = [3,3]\n",
    "    #First conv layer. Fixed: Looks for edges                                                 \n",
    "    print(\"\\n**Working with conv layer 1**\",end=\"\\t\")\n",
    "    print(\"Number of Filters = {}  Kernel size = {}x{}\".format(num_of_filters,kernel_size[0],kernel_size[1]))\n",
    "  \n",
    "    l1_feature_map = conv(img, l1_filter)\n",
    "    print(\"Shape: \",l1_feature_map.shape )\n",
    "        \n",
    "    print(\"\\n**ReLU**\")\n",
    "    l1_feature_map_relu = relu(l1_feature_map)\n",
    "    print(\"Shape: \",l1_feature_map_relu.shape )\n",
    "\n",
    "#     print(\"\\n**Pooling**\")\n",
    "#     l1_feature_map_relu_pool = pooling(l1_feature_map_relu, 2, 2)\n",
    "#     print(\"Shape: \",l1_feature_map_relu_pool.shape )\n",
    "    print(\"**End of conv layer 1**\\n\")\n",
    "\n",
    "    # Second conv layer\n",
    "    num_of_filters = 64\n",
    "    kernel_size = [5,5]\n",
    "    \n",
    "    l2_filter = np.random.rand(num_of_filters, kernel_size[0], kernel_size[1], l1_feature_map_relu.shape[-1])\n",
    "    print(\"\\n**Working with conv layer 2**\",end=\"\\t\")\n",
    "    print(\"Number of Filters = {}  Kernel size = {}x{}\".format(num_of_filters,kernel_size[0],kernel_size[1]))\n",
    "    l2_feature_map = conv(l1_feature_map_relu, l2_filter)\n",
    "    print(\"Shape: \",l2_feature_map.shape )\n",
    "\n",
    "    print(\"\\n**ReLU**\")\n",
    "    l2_feature_map_relu = relu(l2_feature_map)\n",
    "    print(\"Shape: \",l2_feature_map_relu.shape )\n",
    "\n",
    "    print(\"\\n**Pooling**\")\n",
    "    l2_feature_map_relu_pool = pooling(l2_feature_map_relu, 2, 2)\n",
    "    print(\"Shape: \",l2_feature_map_relu_pool.shape )\n",
    "    \n",
    "    print(\"**End of conv layer 2**\\n\")\n",
    "\n",
    "    # Third conv layer\n",
    "    num_of_filters = 128\n",
    "    kernel_size = [7,7]\n",
    "    \n",
    "    l3_filter = np.random.rand(num_of_filters, kernel_size[0], kernel_size[1], l2_feature_map_relu_pool.shape[-1])\n",
    "    print(\"\\n**Working with conv layer 3**\",end=\"\\t\")\n",
    "    print(\"Number of Filters = {}  Kernel size = {}x{}\".format(num_of_filters,kernel_size[0],kernel_size[1]))\n",
    "    l3_feature_map = conv(l2_feature_map_relu_pool, l3_filter)\n",
    "    print(\"Shape: \",l3_feature_map.shape )\n",
    "    \n",
    "    print(\"\\n**ReLU**\")\n",
    "    l3_feature_map_relu = relu(l3_feature_map)\n",
    "    print(\"Shape: \",l3_feature_map_relu.shape )\n",
    "\n",
    "#     print(\"\\n**Pooling**\")\n",
    "#     l3_feature_map_relu_pool = pooling(l3_feature_map_relu, 2, 2)\n",
    "#     print(\"Shape: \",l3_feature_map_relu_pool.shape )\n",
    "\n",
    "    print(\"**End of conv layer 3**\\n\")\n",
    "    \n",
    "    \n",
    "    # Fourth conv layer\n",
    "    num_of_filters = 256\n",
    "    kernel_size = [5,5]\n",
    "    \n",
    "    l4_filter = np.random.rand(num_of_filters, kernel_size[0], kernel_size[1], l3_feature_map_relu.shape[-1])\n",
    "    print(\"\\n**Working with conv layer 4**\",end=\"\\t\")\n",
    "    print(\"Number of Filters = {}  Kernel size = {}x{}\".format(num_of_filters,kernel_size[0],kernel_size[1]))\n",
    "    l4_feature_map = conv(l3_feature_map_relu, l4_filter)\n",
    "    print(\"Shape: \",l4_feature_map.shape )\n",
    "    \n",
    "    print(\"\\n**ReLU**\")\n",
    "    l4_feature_map_relu = relu(l4_feature_map)\n",
    "    print(\"Shape: \",l4_feature_map_relu.shape )\n",
    "\n",
    "    print(\"\\n**Pooling**\")\n",
    "    l4_feature_map_relu_pool = pooling(l4_feature_map_relu, 2, 2)\n",
    "    print(\"Shape: \",l4_feature_map_relu_pool.shape )\n",
    "\n",
    "    print(\"**End of conv layer 4**\\n\")    \n",
    "    \n",
    "    \n",
    "    # Fifth conv layer\n",
    "    num_of_filters = 512\n",
    "    kernel_size = [3,3]\n",
    "    \n",
    "    l5_filter = np.random.rand(num_of_filters, kernel_size[0], kernel_size[1], l4_feature_map_relu_pool.shape[-1])\n",
    "    print(\"\\n**Working with conv layer 4**\",end=\"\\t\")\n",
    "    print(\"Number of Filters = {}  Kernel size = {}x{}\".format(num_of_filters,kernel_size[0],kernel_size[1]))\n",
    "    l5_feature_map = conv(l4_feature_map_relu_pool, l5_filter)\n",
    "    print(\"Shape: \",l5_feature_map.shape )\n",
    "    \n",
    "    print(\"\\n**ReLU**\")\n",
    "    l5_feature_map_relu = relu(l5_feature_map)\n",
    "    print(\"Shape: \",l5_feature_map_relu.shape )\n",
    "\n",
    "#     print(\"\\n**Pooling**\")-\n",
    "#     l5_feature_map_relu_pool = pooling(l4_feature_map_relu, 2, 2)\n",
    "#     print(\"Shape: \",l4_feature_map_relu_pool.shape )\n",
    "\n",
    "    \n",
    "    print(\"**End of conv layer 5**\\n\")    \n",
    "    last_feature_map = l5_feature_map_relu\n",
    "    return last_feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_feature_map = stack_cnn_layers(img,l1_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (120, 120, 3)\n",
      "\n",
      "**Working with conv layer 1**\tNumber of Filters = 2  Kernel size = 3x3\n",
      "Filter  1\n",
      "Filter  2\n",
      "Shape:  (118, 118, 2)\n",
      "\n",
      "**ReLU**\n",
      "Shape:  (118, 118, 2)\n",
      "**End of conv layer 1**\n",
      "\n",
      "\n",
      "**Working with conv layer 2**\tNumber of Filters = 64  Kernel size = 5x5\n",
      "Filter  1\n",
      "Filter  2\n",
      "Filter  3\n",
      "Filter  4\n",
      "Filter  5\n",
      "Filter  6\n",
      "Filter  7\n",
      "Filter  8\n",
      "Filter  9\n",
      "Filter  10\n",
      "Filter  11\n",
      "Filter  12\n",
      "Filter  13\n",
      "Filter  14\n",
      "Filter  15\n",
      "Filter  16\n",
      "Filter  17\n",
      "Filter  18\n",
      "Filter  19\n",
      "Filter  20\n",
      "Filter  21\n",
      "Filter  22\n",
      "Filter  23\n",
      "Filter  24\n",
      "Filter  25\n",
      "Filter  26\n",
      "Filter  27\n",
      "Filter  28\n",
      "Filter  29\n",
      "Filter  30\n",
      "Filter  31\n",
      "Filter  32\n",
      "Filter  33\n",
      "Filter  34\n",
      "Filter  35\n",
      "Filter  36\n",
      "Filter  37\n",
      "Filter  38\n",
      "Filter  39\n",
      "Filter  40\n",
      "Filter  41\n",
      "Filter  42\n",
      "Filter  43\n",
      "Filter  44\n",
      "Filter  45\n",
      "Filter  46\n",
      "Filter  47\n",
      "Filter  48\n",
      "Filter  49\n",
      "Filter  50\n",
      "Filter  51\n",
      "Filter  52\n",
      "Filter  53\n",
      "Filter  54\n",
      "Filter  55\n",
      "Filter  56\n",
      "Filter  57\n",
      "Filter  58\n",
      "Filter  59\n",
      "Filter  60\n",
      "Filter  61\n",
      "Filter  62\n",
      "Filter  63\n",
      "Filter  64\n",
      "Shape:  (114, 114, 64)\n",
      "\n",
      "**ReLU**\n",
      "Shape:  (114, 114, 64)\n",
      "\n",
      "**Pooling**\n",
      "Shape:  (57, 57, 64)\n",
      "**End of conv layer 2**\n",
      "\n",
      "\n",
      "**Working with conv layer 3**\tNumber of Filters = 128  Kernel size = 7x7\n",
      "Filter  1\n",
      "Filter  2\n",
      "Filter  3\n",
      "Filter  4\n",
      "Filter  5\n",
      "Filter  6\n",
      "Filter  7\n",
      "Filter  8\n",
      "Filter  9\n",
      "Filter  10\n",
      "Filter  11\n",
      "Filter  12\n",
      "Filter  13\n",
      "Filter  14\n",
      "Filter  15\n",
      "Filter  16\n",
      "Filter  17\n",
      "Filter  18\n",
      "Filter  19\n",
      "Filter  20\n",
      "Filter  21\n",
      "Filter  22\n",
      "Filter  23\n",
      "Filter  24\n",
      "Filter  25\n",
      "Filter  26\n",
      "Filter  27\n",
      "Filter  28\n",
      "Filter  29\n",
      "Filter  30\n",
      "Filter  31\n",
      "Filter  32\n",
      "Filter  33\n",
      "Filter  34\n",
      "Filter  35\n",
      "Filter  36\n",
      "Filter  37\n",
      "Filter  38\n",
      "Filter  39\n",
      "Filter  40\n",
      "Filter  41\n",
      "Filter  42\n",
      "Filter  43\n",
      "Filter  44\n",
      "Filter  45\n",
      "Filter  46\n",
      "Filter  47\n",
      "Filter  48\n",
      "Filter  49\n",
      "Filter  50\n",
      "Filter  51\n",
      "Filter  52\n",
      "Filter  53\n",
      "Filter  54\n",
      "Filter  55\n",
      "Filter  56\n",
      "Filter  57\n",
      "Filter  58\n",
      "Filter  59\n",
      "Filter  60\n",
      "Filter  61\n",
      "Filter  62\n",
      "Filter  63\n",
      "Filter  64\n",
      "Filter  65\n",
      "Filter  66\n",
      "Filter  67\n",
      "Filter  68\n",
      "Filter  69\n",
      "Filter  70\n",
      "Filter  71\n",
      "Filter  72\n",
      "Filter  73\n",
      "Filter  74\n",
      "Filter  75\n",
      "Filter  76\n",
      "Filter  77\n",
      "Filter  78\n",
      "Filter  79\n",
      "Filter  80\n",
      "Filter  81\n",
      "Filter  82\n",
      "Filter  83\n",
      "Filter  84\n",
      "Filter  85\n",
      "Filter  86\n",
      "Filter  87\n",
      "Filter  88\n",
      "Filter  89\n",
      "Filter  90\n",
      "Filter  91\n",
      "Filter  92\n",
      "Filter  93\n",
      "Filter  94\n",
      "Filter  95\n",
      "Filter  96\n",
      "Filter  97\n",
      "Filter  98\n",
      "Filter  99\n",
      "Filter  100\n",
      "Filter  101\n",
      "Filter  102\n",
      "Filter  103\n",
      "Filter  104\n",
      "Filter  105\n",
      "Filter  106\n",
      "Filter  107\n",
      "Filter  108\n",
      "Filter  109\n",
      "Filter  110\n",
      "Filter  111\n",
      "Filter  112\n",
      "Filter  113\n",
      "Filter  114\n",
      "Filter  115\n",
      "Filter  116\n",
      "Filter  117\n",
      "Filter  118\n",
      "Filter  119\n",
      "Filter  120\n",
      "Filter  121\n",
      "Filter  122\n",
      "Filter  123\n",
      "Filter  124\n",
      "Filter  125\n",
      "Filter  126\n",
      "Filter  127\n",
      "Filter  128\n",
      "Shape:  (51, 51, 128)\n",
      "\n",
      "**ReLU**\n",
      "Shape:  (51, 51, 128)\n",
      "**End of conv layer 3**\n",
      "\n",
      "\n",
      "**Working with conv layer 4**\tNumber of Filters = 256  Kernel size = 5x5\n",
      "Filter  1\n",
      "Filter  2\n",
      "Filter  3\n",
      "Filter  4\n",
      "Filter  5\n",
      "Filter  6\n",
      "Filter  7\n",
      "Filter  8\n",
      "Filter  9\n",
      "Filter  10\n",
      "Filter  11\n",
      "Filter  12\n",
      "Filter  13\n",
      "Filter  14\n",
      "Filter  15\n",
      "Filter  16\n",
      "Filter  17\n",
      "Filter  18\n",
      "Filter  19\n",
      "Filter  20\n",
      "Filter  21\n",
      "Filter  22\n",
      "Filter  23\n",
      "Filter  24\n",
      "Filter  25\n",
      "Filter  26\n",
      "Filter  27\n",
      "Filter  28\n",
      "Filter  29\n",
      "Filter  30\n",
      "Filter  31\n",
      "Filter  32\n",
      "Filter  33\n",
      "Filter  34\n",
      "Filter  35\n",
      "Filter  36\n",
      "Filter  37\n",
      "Filter  38\n",
      "Filter  39\n",
      "Filter  40\n",
      "Filter  41\n",
      "Filter  42\n",
      "Filter  43\n",
      "Filter  44\n",
      "Filter  45\n",
      "Filter  46\n",
      "Filter  47\n",
      "Filter  48\n",
      "Filter  49\n",
      "Filter  50\n",
      "Filter  51\n",
      "Filter  52\n",
      "Filter  53\n",
      "Filter  54\n",
      "Filter  55\n",
      "Filter  56\n",
      "Filter  57\n",
      "Filter  58\n",
      "Filter  59\n",
      "Filter  60\n",
      "Filter  61\n",
      "Filter  62\n",
      "Filter  63\n",
      "Filter  64\n",
      "Filter  65\n",
      "Filter  66\n",
      "Filter  67\n",
      "Filter  68\n",
      "Filter  69\n",
      "Filter  70\n",
      "Filter  71\n",
      "Filter  72\n",
      "Filter  73\n",
      "Filter  74\n",
      "Filter  75\n",
      "Filter  76\n",
      "Filter  77\n",
      "Filter  78\n",
      "Filter  79\n",
      "Filter  80\n",
      "Filter  81\n",
      "Filter  82\n",
      "Filter  83\n",
      "Filter  84\n",
      "Filter  85\n",
      "Filter  86\n",
      "Filter  87\n",
      "Filter  88\n",
      "Filter  89\n",
      "Filter  90\n",
      "Filter  91\n",
      "Filter  92\n",
      "Filter  93\n",
      "Filter  94\n",
      "Filter  95\n",
      "Filter  96\n",
      "Filter  97\n",
      "Filter  98\n",
      "Filter  99\n",
      "Filter  100\n",
      "Filter  101\n",
      "Filter  102\n",
      "Filter  103\n",
      "Filter  104\n",
      "Filter  105\n",
      "Filter  106\n",
      "Filter  107\n",
      "Filter  108\n",
      "Filter  109\n",
      "Filter  110\n",
      "Filter  111\n",
      "Filter  112\n",
      "Filter  113\n",
      "Filter  114\n",
      "Filter  115\n",
      "Filter  116\n",
      "Filter  117\n",
      "Filter  118\n",
      "Filter  119\n",
      "Filter  120\n",
      "Filter  121\n",
      "Filter  122\n",
      "Filter  123\n",
      "Filter  124\n",
      "Filter  125\n",
      "Filter  126\n",
      "Filter  127\n",
      "Filter  128\n",
      "Filter  129\n",
      "Filter  130\n",
      "Filter  131\n",
      "Filter  132\n",
      "Filter  133\n",
      "Filter  134\n",
      "Filter  135\n",
      "Filter  136\n",
      "Filter  137\n",
      "Filter  138\n",
      "Filter  139\n",
      "Filter  140\n",
      "Filter  141\n",
      "Filter  142\n",
      "Filter  143\n",
      "Filter  144\n",
      "Filter  145\n",
      "Filter  146\n",
      "Filter  147\n",
      "Filter  148\n",
      "Filter  149\n",
      "Filter  150\n",
      "Filter  151\n",
      "Filter  152\n",
      "Filter  153\n",
      "Filter  154\n",
      "Filter  155\n",
      "Filter  156\n",
      "Filter  157\n",
      "Filter  158\n",
      "Filter  159\n",
      "Filter  160\n",
      "Filter  161\n",
      "Filter  162\n",
      "Filter  163\n",
      "Filter  164\n",
      "Filter  165\n",
      "Filter  166\n",
      "Filter  167\n",
      "Filter  168\n",
      "Filter  169\n",
      "Filter  170\n",
      "Filter  171\n",
      "Filter  172\n",
      "Filter  173\n",
      "Filter  174\n",
      "Filter  175\n",
      "Filter  176\n",
      "Filter  177\n",
      "Filter  178\n",
      "Filter  179\n",
      "Filter  180\n",
      "Filter  181\n",
      "Filter  182\n",
      "Filter  183\n",
      "Filter  184\n",
      "Filter  185\n",
      "Filter  186\n",
      "Filter  187\n",
      "Filter  188\n",
      "Filter  189\n",
      "Filter  190\n",
      "Filter  191\n",
      "Filter  192\n",
      "Filter  193\n",
      "Filter  194\n",
      "Filter  195\n",
      "Filter  196\n",
      "Filter  197\n",
      "Filter  198\n",
      "Filter  199\n",
      "Filter  200\n",
      "Filter  201\n",
      "Filter  202\n",
      "Filter  203\n",
      "Filter  204\n",
      "Filter  205\n",
      "Filter  206\n",
      "Filter  207\n",
      "Filter  208\n",
      "Filter  209\n",
      "Filter  210\n",
      "Filter  211\n",
      "Filter  212\n",
      "Filter  213\n",
      "Filter  214\n",
      "Filter  215\n",
      "Filter  216\n",
      "Filter  217\n",
      "Filter  218\n",
      "Filter  219\n",
      "Filter  220\n",
      "Filter  221\n",
      "Filter  222\n",
      "Filter  223\n",
      "Filter  224\n",
      "Filter  225\n",
      "Filter  226\n",
      "Filter  227\n",
      "Filter  228\n",
      "Filter  229\n",
      "Filter  230\n",
      "Filter  231\n",
      "Filter  232\n",
      "Filter  233\n",
      "Filter  234\n",
      "Filter  235\n",
      "Filter  236\n",
      "Filter  237\n",
      "Filter  238\n",
      "Filter  239\n",
      "Filter  240\n",
      "Filter  241\n",
      "Filter  242\n",
      "Filter  243\n",
      "Filter  244\n",
      "Filter  245\n",
      "Filter  246\n",
      "Filter  247\n",
      "Filter  248\n",
      "Filter  249\n",
      "Filter  250\n",
      "Filter  251\n",
      "Filter  252\n",
      "Filter  253\n",
      "Filter  254\n",
      "Filter  255\n",
      "Filter  256\n",
      "Shape:  (47, 47, 256)\n",
      "\n",
      "**ReLU**\n",
      "Shape:  (47, 47, 256)\n",
      "\n",
      "**Pooling**\n",
      "Shape:  (24, 24, 256)\n",
      "**End of conv layer 4**\n",
      "\n",
      "\n",
      "**Working with conv layer 4**\tNumber of Filters = 512  Kernel size = 3x3\n",
      "Filter  1\n",
      "Filter  2\n",
      "Filter  3\n",
      "Filter  4\n",
      "Filter  5\n",
      "Filter  6\n",
      "Filter  7\n",
      "Filter  8\n",
      "Filter  9\n",
      "Filter  10\n",
      "Filter  11\n",
      "Filter  12\n",
      "Filter  13\n",
      "Filter  14\n",
      "Filter  15\n",
      "Filter  16\n",
      "Filter  17\n",
      "Filter  18\n",
      "Filter  19\n",
      "Filter  20\n",
      "Filter  21\n",
      "Filter  22\n",
      "Filter  23\n",
      "Filter  24\n",
      "Filter  25\n",
      "Filter  26\n",
      "Filter  27\n",
      "Filter  28\n",
      "Filter  29\n",
      "Filter  30\n",
      "Filter  31\n",
      "Filter  32\n",
      "Filter  33\n",
      "Filter  34\n",
      "Filter  35\n",
      "Filter  36\n",
      "Filter  37\n",
      "Filter  38\n",
      "Filter  39\n",
      "Filter  40\n",
      "Filter  41\n",
      "Filter  42\n",
      "Filter  43\n",
      "Filter  44\n",
      "Filter  45\n",
      "Filter  46\n",
      "Filter  47\n",
      "Filter  48\n",
      "Filter  49\n",
      "Filter  50\n",
      "Filter  51\n",
      "Filter  52\n",
      "Filter  53\n",
      "Filter  54\n",
      "Filter  55\n",
      "Filter  56\n",
      "Filter  57\n",
      "Filter  58\n",
      "Filter  59\n",
      "Filter  60\n",
      "Filter  61\n",
      "Filter  62\n",
      "Filter  63\n",
      "Filter  64\n",
      "Filter  65\n",
      "Filter  66\n",
      "Filter  67\n",
      "Filter  68\n",
      "Filter  69\n",
      "Filter  70\n",
      "Filter  71\n",
      "Filter  72\n",
      "Filter  73\n",
      "Filter  74\n",
      "Filter  75\n",
      "Filter  76\n",
      "Filter  77\n",
      "Filter  78\n",
      "Filter  79\n",
      "Filter  80\n",
      "Filter  81\n",
      "Filter  82\n",
      "Filter  83\n",
      "Filter  84\n",
      "Filter  85\n",
      "Filter  86\n",
      "Filter  87\n",
      "Filter  88\n",
      "Filter  89\n",
      "Filter  90\n",
      "Filter  91\n",
      "Filter  92\n",
      "Filter  93\n",
      "Filter  94\n",
      "Filter  95\n",
      "Filter  96\n",
      "Filter  97\n",
      "Filter  98\n",
      "Filter  99\n",
      "Filter  100\n",
      "Filter  101\n",
      "Filter  102\n",
      "Filter  103\n",
      "Filter  104\n",
      "Filter  105\n",
      "Filter  106\n",
      "Filter  107\n",
      "Filter  108\n",
      "Filter  109\n",
      "Filter  110\n",
      "Filter  111\n",
      "Filter  112\n",
      "Filter  113\n",
      "Filter  114\n",
      "Filter  115\n",
      "Filter  116\n",
      "Filter  117\n",
      "Filter  118\n",
      "Filter  119\n",
      "Filter  120\n",
      "Filter  121\n",
      "Filter  122\n",
      "Filter  123\n",
      "Filter  124\n",
      "Filter  125\n",
      "Filter  126\n",
      "Filter  127\n",
      "Filter  128\n",
      "Filter  129\n",
      "Filter  130\n",
      "Filter  131\n",
      "Filter  132\n",
      "Filter  133\n",
      "Filter  134\n",
      "Filter  135\n",
      "Filter  136\n",
      "Filter  137\n",
      "Filter  138\n",
      "Filter  139\n",
      "Filter  140\n",
      "Filter  141\n",
      "Filter  142\n",
      "Filter  143\n",
      "Filter  144\n",
      "Filter  145\n",
      "Filter  146\n",
      "Filter  147\n",
      "Filter  148\n",
      "Filter  149\n",
      "Filter  150\n",
      "Filter  151\n",
      "Filter  152\n",
      "Filter  153\n",
      "Filter  154\n",
      "Filter  155\n",
      "Filter  156\n",
      "Filter  157\n",
      "Filter  158\n",
      "Filter  159\n",
      "Filter  160\n",
      "Filter  161\n",
      "Filter  162\n",
      "Filter  163\n",
      "Filter  164\n",
      "Filter  165\n",
      "Filter  166\n",
      "Filter  167\n",
      "Filter  168\n",
      "Filter  169\n",
      "Filter  170\n",
      "Filter  171\n",
      "Filter  172\n",
      "Filter  173\n",
      "Filter  174\n",
      "Filter  175\n",
      "Filter  176\n",
      "Filter  177\n",
      "Filter  178\n",
      "Filter  179\n",
      "Filter  180\n",
      "Filter  181\n",
      "Filter  182\n",
      "Filter  183\n",
      "Filter  184\n",
      "Filter  185\n",
      "Filter  186\n",
      "Filter  187\n",
      "Filter  188\n",
      "Filter  189\n",
      "Filter  190\n",
      "Filter  191\n",
      "Filter  192\n",
      "Filter  193\n",
      "Filter  194\n",
      "Filter  195\n",
      "Filter  196\n",
      "Filter  197\n",
      "Filter  198\n",
      "Filter  199\n",
      "Filter  200\n",
      "Filter  201\n",
      "Filter  202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter  203\n",
      "Filter  204\n",
      "Filter  205\n",
      "Filter  206\n",
      "Filter  207\n",
      "Filter  208\n",
      "Filter  209\n",
      "Filter  210\n",
      "Filter  211\n",
      "Filter  212\n",
      "Filter  213\n",
      "Filter  214\n",
      "Filter  215\n",
      "Filter  216\n",
      "Filter  217\n",
      "Filter  218\n",
      "Filter  219\n",
      "Filter  220\n",
      "Filter  221\n",
      "Filter  222\n",
      "Filter  223\n",
      "Filter  224\n",
      "Filter  225\n",
      "Filter  226\n",
      "Filter  227\n",
      "Filter  228\n",
      "Filter  229\n",
      "Filter  230\n",
      "Filter  231\n",
      "Filter  232\n",
      "Filter  233\n",
      "Filter  234\n",
      "Filter  235\n",
      "Filter  236\n",
      "Filter  237\n",
      "Filter  238\n",
      "Filter  239\n",
      "Filter  240\n",
      "Filter  241\n",
      "Filter  242\n",
      "Filter  243\n",
      "Filter  244\n",
      "Filter  245\n",
      "Filter  246\n",
      "Filter  247\n",
      "Filter  248\n",
      "Filter  249\n",
      "Filter  250\n",
      "Filter  251\n",
      "Filter  252\n",
      "Filter  253\n",
      "Filter  254\n",
      "Filter  255\n",
      "Filter  256\n",
      "Filter  257\n",
      "Filter  258\n",
      "Filter  259\n",
      "Filter  260\n",
      "Filter  261\n",
      "Filter  262\n",
      "Filter  263\n",
      "Filter  264\n",
      "Filter  265\n",
      "Filter  266\n",
      "Filter  267\n",
      "Filter  268\n",
      "Filter  269\n",
      "Filter  270\n",
      "Filter  271\n",
      "Filter  272\n",
      "Filter  273\n",
      "Filter  274\n",
      "Filter  275\n",
      "Filter  276\n",
      "Filter  277\n",
      "Filter  278\n",
      "Filter  279\n",
      "Filter  280\n",
      "Filter  281\n",
      "Filter  282\n",
      "Filter  283\n",
      "Filter  284\n",
      "Filter  285\n",
      "Filter  286\n",
      "Filter  287\n",
      "Filter  288\n",
      "Filter  289\n",
      "Filter  290\n",
      "Filter  291\n",
      "Filter  292\n",
      "Filter  293\n",
      "Filter  294\n",
      "Filter  295\n",
      "Filter  296\n",
      "Filter  297\n",
      "Filter  298\n",
      "Filter  299\n",
      "Filter  300\n",
      "Filter  301\n",
      "Filter  302\n",
      "Filter  303\n",
      "Filter  304\n",
      "Filter  305\n",
      "Filter  306\n",
      "Filter  307\n",
      "Filter  308\n",
      "Filter  309\n",
      "Filter  310\n",
      "Filter  311\n",
      "Filter  312\n",
      "Filter  313\n",
      "Filter  314\n",
      "Filter  315\n",
      "Filter  316\n",
      "Filter  317\n",
      "Filter  318\n",
      "Filter  319\n",
      "Filter  320\n",
      "Filter  321\n",
      "Filter  322\n",
      "Filter  323\n",
      "Filter  324\n",
      "Filter  325\n",
      "Filter  326\n",
      "Filter  327\n",
      "Filter  328\n",
      "Filter  329\n",
      "Filter  330\n",
      "Filter  331\n",
      "Filter  332\n",
      "Filter  333\n",
      "Filter  334\n",
      "Filter  335\n",
      "Filter  336\n",
      "Filter  337\n",
      "Filter  338\n",
      "Filter  339\n",
      "Filter  340\n",
      "Filter  341\n",
      "Filter  342\n",
      "Filter  343\n",
      "Filter  344\n",
      "Filter  345\n",
      "Filter  346\n",
      "Filter  347\n",
      "Filter  348\n",
      "Filter  349\n",
      "Filter  350\n",
      "Filter  351\n",
      "Filter  352\n",
      "Filter  353\n",
      "Filter  354\n",
      "Filter  355\n",
      "Filter  356\n",
      "Filter  357\n",
      "Filter  358\n",
      "Filter  359\n",
      "Filter  360\n",
      "Filter  361\n",
      "Filter  362\n",
      "Filter  363\n",
      "Filter  364\n",
      "Filter  365\n",
      "Filter  366\n",
      "Filter  367\n",
      "Filter  368\n",
      "Filter  369\n",
      "Filter  370\n",
      "Filter  371\n",
      "Filter  372\n",
      "Filter  373\n",
      "Filter  374\n",
      "Filter  375\n",
      "Filter  376\n",
      "Filter  377\n",
      "Filter  378\n",
      "Filter  379\n",
      "Filter  380\n",
      "Filter  381\n",
      "Filter  382\n",
      "Filter  383\n",
      "Filter  384\n",
      "Filter  385\n",
      "Filter  386\n",
      "Filter  387\n",
      "Filter  388\n",
      "Filter  389\n",
      "Filter  390\n",
      "Filter  391\n",
      "Filter  392\n",
      "Filter  393\n",
      "Filter  394\n",
      "Filter  395\n",
      "Filter  396\n",
      "Filter  397\n",
      "Filter  398\n",
      "Filter  399\n",
      "Filter  400\n",
      "Filter  401\n",
      "Filter  402\n",
      "Filter  403\n",
      "Filter  404\n",
      "Filter  405\n",
      "Filter  406\n",
      "Filter  407\n",
      "Filter  408\n",
      "Filter  409\n",
      "Filter  410\n",
      "Filter  411\n",
      "Filter  412\n",
      "Filter  413\n",
      "Filter  414\n",
      "Filter  415\n",
      "Filter  416\n",
      "Filter  417\n",
      "Filter  418\n",
      "Filter  419\n",
      "Filter  420\n",
      "Filter  421\n",
      "Filter  422\n",
      "Filter  423\n",
      "Filter  424\n",
      "Filter  425\n",
      "Filter  426\n",
      "Filter  427\n",
      "Filter  428\n",
      "Filter  429\n",
      "Filter  430\n",
      "Filter  431\n",
      "Filter  432\n",
      "Filter  433\n",
      "Filter  434\n",
      "Filter  435\n",
      "Filter  436\n",
      "Filter  437\n",
      "Filter  438\n",
      "Filter  439\n",
      "Filter  440\n",
      "Filter  441\n",
      "Filter  442\n",
      "Filter  443\n",
      "Filter  444\n",
      "Filter  445\n",
      "Filter  446\n",
      "Filter  447\n",
      "Filter  448\n",
      "Filter  449\n",
      "Filter  450\n",
      "Filter  451\n",
      "Filter  452\n",
      "Filter  453\n",
      "Filter  454\n",
      "Filter  455\n",
      "Filter  456\n",
      "Filter  457\n",
      "Filter  458\n",
      "Filter  459\n",
      "Filter  460\n",
      "Filter  461\n",
      "Filter  462\n",
      "Filter  463\n",
      "Filter  464\n",
      "Filter  465\n",
      "Filter  466\n",
      "Filter  467\n",
      "Filter  468\n",
      "Filter  469\n",
      "Filter  470\n",
      "Filter  471\n",
      "Filter  472\n",
      "Filter  473\n",
      "Filter  474\n",
      "Filter  475\n",
      "Filter  476\n",
      "Filter  477\n",
      "Filter  478\n",
      "Filter  479\n",
      "Filter  480\n",
      "Filter  481\n",
      "Filter  482\n",
      "Filter  483\n",
      "Filter  484\n",
      "Filter  485\n",
      "Filter  486\n",
      "Filter  487\n",
      "Filter  488\n",
      "Filter  489\n",
      "Filter  490\n",
      "Filter  491\n",
      "Filter  492\n",
      "Filter  493\n",
      "Filter  494\n",
      "Filter  495\n",
      "Filter  496\n",
      "Filter  497\n",
      "Filter  498\n",
      "Filter  499\n",
      "Filter  500\n",
      "Filter  501\n",
      "Filter  502\n",
      "Filter  503\n",
      "Filter  504\n",
      "Filter  505\n",
      "Filter  506\n",
      "Filter  507\n",
      "Filter  508\n",
      "Filter  509\n",
      "Filter  510\n",
      "Filter  511\n",
      "Filter  512\n",
      "Shape:  (22, 22, 512)\n",
      "\n",
      "**ReLU**\n",
      "Shape:  (22, 22, 512)\n",
      "**End of conv layer 5**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minibatch_l1_feature_map = []\n",
    "for img in current_batch[0:1]:\n",
    "    last_feature_map = stack_cnn_layers(img,l1_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 22, 512)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(feature_map):\n",
    "    total = 1\n",
    "    for dim in feature_map.shape:\n",
    "        total = total * dim\n",
    "    \n",
    "    flattened = feature_map.reshape(total,1)\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247808, 1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = flatten(last_feature_map)\n",
    "flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_indices = np.random.randint(flattened.shape[0],size = 512)\n",
    "chosen_indices.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = flattened[chosen_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"chosen_features_CNN.csv\",chosen_features,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Functions\n",
    "#sigmoid\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "#tanh activation\n",
    "def tanh_activation(X):\n",
    "    return np.tanh(X)\n",
    "\n",
    "#softmax activation\n",
    "def softmax(X):\n",
    "    exp_X = np.exp(X)\n",
    "    exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)\n",
    "    exp_X = exp_X/exp_X_sum\n",
    "    return exp_X\n",
    "\n",
    "#derivative of tanh\n",
    "def tanh_derivative(X):\n",
    "    return 1-(X**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameters\n",
    "def initialize_parameters():\n",
    "    #initialize the parameters with 0 mean and 0.01 standard deviation\n",
    "    mean = 0\n",
    "    std = 0.01\n",
    "    \n",
    "    #lstm cell weights\n",
    "    forget_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    input_gate_weights  = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    output_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    gate_gate_weights   = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n",
    "    \n",
    "    #hidden to output weights (output cell)\n",
    "    hidden_output_weights = np.random.normal(mean,std,(hidden_units,output_units))\n",
    "    \n",
    "    parameters = dict()\n",
    "    parameters['fgw'] = forget_gate_weights\n",
    "    parameters['igw'] = input_gate_weights\n",
    "    parameters['ogw'] = output_gate_weights\n",
    "    parameters['ggw'] = gate_gate_weights\n",
    "    parameters['how'] = hidden_output_weights\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single lstm cell\n",
    "def lstm_cell(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters):\n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    \n",
    "    #concat batch data and prev_activation matrix\n",
    "    concat_dataset = np.concatenate((batch_dataset,prev_activation_matrix),axis=1)\n",
    "    \n",
    "    #forget gate activations\n",
    "    fa = np.matmul(concat_dataset,fgw)\n",
    "    fa = sigmoid(fa)\n",
    "    \n",
    "    #input gate activations\n",
    "    ia = np.matmul(concat_dataset,igw)\n",
    "    ia = sigmoid(ia)\n",
    "    \n",
    "    #output gate activations\n",
    "    oa = np.matmul(concat_dataset,ogw)\n",
    "    oa = sigmoid(oa)\n",
    "    \n",
    "    #gate gate activations\n",
    "    ga = np.matmul(concat_dataset,ggw)\n",
    "    ga = tanh_activation(ga)\n",
    "    \n",
    "    #new cell memory matrix\n",
    "    cell_memory_matrix = np.multiply(fa,prev_cell_matrix) + np.multiply(ia,ga)\n",
    "    \n",
    "    #current activation matrix\n",
    "    activation_matrix = np.multiply(oa, tanh_activation(cell_memory_matrix))\n",
    "    \n",
    "    #lets store the activations to be used in back prop\n",
    "    lstm_activations = dict()\n",
    "    lstm_activations['fa'] = fa\n",
    "    lstm_activations['ia'] = ia\n",
    "    lstm_activations['oa'] = oa\n",
    "    lstm_activations['ga'] = ga\n",
    "    \n",
    "    return lstm_activations,cell_memory_matrix,activation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_cell(activation_matrix,parameters):\n",
    "    #get hidden to output parameters\n",
    "    how = parameters['how']\n",
    "    \n",
    "    #get outputs \n",
    "    output_matrix = np.matmul(activation_matrix,how)\n",
    "    output_matrix = softmax(output_matrix)\n",
    "    \n",
    "    return output_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get corresponding embeddings for the batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(batch_dataset,embeddings):\n",
    "    embedding_dataset = np.matmul(batch_dataset,embeddings)\n",
    "    return embedding_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "![](https://github.com/navjindervirdee/neural-networks/blob/master/Recurrent%20Neural%20Network/LSTMForward.JPG?raw=true)\n",
    "\n",
    "Function returns the intermediate ativations in the respective caches:\n",
    "* LSTM Cache :- All lstm cell activation in every cell (fa,ia,ga,oa)\n",
    "* Activation Cache : All activation (a0,a1,a2..)\n",
    "* Cell Cache : All cell activations (c0,c1,c2..\n",
    "* Embedding cache : Embeddings of each batch (e0,e1,e2..)\n",
    "* Output Cache : All output (o1,o2,o3... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "def forward_propagation(batches,parameters,embeddings):\n",
    "    #get batch size\n",
    "    batch_size = batches[0].shape[0]\n",
    "    \n",
    "    #to store the activations of all the unrollings.\n",
    "    lstm_cache = dict()                 #lstm cache\n",
    "    activation_cache = dict()           #activation cache \n",
    "    cell_cache = dict()                 #cell cache\n",
    "    output_cache = dict()               #output cache\n",
    "    embedding_cache = dict()            #embedding cache \n",
    "    \n",
    "    #initial activation_matrix(a0) and cell_matrix(c0)\n",
    "    a0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n",
    "    c0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n",
    "    \n",
    "    #store the initial activations in cache\n",
    "    activation_cache['a0'] = a0\n",
    "    cell_cache['c0'] = c0\n",
    "    \n",
    "    #unroll the names\n",
    "    for i in range(len(batches)-1):\n",
    "        #get first first character batch\n",
    "        batch_dataset = batches[i]\n",
    "        \n",
    "        #get embeddings \n",
    "        batch_dataset = get_embeddings(batch_dataset,embeddings)\n",
    "        embedding_cache['emb'+str(i)] = batch_dataset\n",
    "        \n",
    "        #lstm cell\n",
    "        lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n",
    "        \n",
    "        #output cell\n",
    "        ot = output_cell(at,parameters)\n",
    "        \n",
    "        #store the time 't' activations in caches\n",
    "        lstm_cache['lstm' + str(i+1)]  = lstm_activations\n",
    "        activation_cache['a'+str(i+1)] = at\n",
    "        cell_cache['c' + str(i+1)] = ct\n",
    "        output_cache['o'+str(i+1)] = ot\n",
    "        \n",
    "        #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n",
    "        a0 = at\n",
    "        c0 = ct\n",
    "        \n",
    "    return embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Loss, Perplexity, and Accuracy\n",
    "**Loss**\n",
    "* Loss at time t = -sum(Y x log(d) + (1-Y) x log(1-pred)))/m\n",
    "* Overall Loss = **∑**(Loss(t)) sum of all losses at each time step 't'\n",
    "\n",
    "**Perplexity **\n",
    "* Probability Product = **∏**(prob(pred_char)) for each char in name\n",
    "* Perplexity = (1/probability_product) ^ (1/n) where n in number of chars in name\n",
    "\n",
    "**Accuracy**\n",
    "* Accuracy(t) = (Y==predictions,axis=1) for all time steps\n",
    "* Accuracy = ((**∑**Acc(t))/batch_size)/n for all time steps, n is number of chars in name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate loss, perplexity and accuracy\n",
    "def cal_loss_accuracy(batch_labels,output_cache):\n",
    "    loss = 0  #to sum loss for each time step\n",
    "    acc  = 0  #to sum acc for each time step \n",
    "    prob = 1  #probability product of each time step predicted char\n",
    "    \n",
    "    #batch size\n",
    "    batch_size = batch_labels[0].shape[0]\n",
    "    \n",
    "    #loop through each time step\n",
    "    for i in range(1,len(output_cache)+1):\n",
    "        #get true labels and predictions\n",
    "        labels = batch_labels[i]\n",
    "        pred = output_cache['o'+str(i)]\n",
    "        \n",
    "        prob = np.multiply(prob,np.sum(np.multiply(labels,pred),axis=1).reshape(-1,1))\n",
    "        loss += np.sum((np.multiply(labels,np.log(pred)) + np.multiply(1-labels,np.log(1-pred))),axis=1).reshape(-1,1)\n",
    "        acc  += np.array(np.argmax(labels,1)==np.argmax(pred,1),dtype=np.float32).reshape(-1,1)\n",
    "    \n",
    "    #calculate perplexity loss and accuracy\n",
    "    perplexity = np.sum((1/prob)**(1/len(output_cache)))/batch_size\n",
    "    loss = np.sum(loss)*(-1/batch_size)\n",
    "    acc  = np.sum(acc)/(batch_size)\n",
    "    acc = acc/len(output_cache)\n",
    "    \n",
    "    return perplexity,loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Output Cell Errors for each time step\n",
    "* Output Error Cache :- to store output error for each time step\n",
    "* Activation Error Cache : to store activation error for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate output cell errors\n",
    "def calculate_output_cell_error(batch_labels,output_cache,parameters):\n",
    "    #to store the output errors for each time step\n",
    "    output_error_cache = dict()\n",
    "    activation_error_cache = dict()\n",
    "    how = parameters['how']\n",
    "    \n",
    "    #loop through each time step\n",
    "    for i in range(1,len(output_cache)+1):\n",
    "        #get true and predicted labels\n",
    "        labels = batch_labels[i]\n",
    "        pred = output_cache['o'+str(i)]\n",
    "        \n",
    "        #calculate the output_error for time step 't'\n",
    "        error_output = pred - labels\n",
    "        \n",
    "        #calculate the activation error for time step 't'\n",
    "        error_activation = np.matmul(error_output,how.T)\n",
    "        \n",
    "        #store the output and activation error in dict\n",
    "        output_error_cache['eo'+str(i)] = error_output\n",
    "        activation_error_cache['ea'+str(i)] = error_activation\n",
    "        \n",
    "    return output_error_cache,activation_error_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate error for single lstm cell\n",
    "def calculate_single_lstm_cell_error(activation_output_error,next_activation_error,next_cell_error,parameters,lstm_activation,cell_activation,prev_cell_activation):\n",
    "    #activation error =  error coming from output cell and error coming from the next lstm cell\n",
    "    activation_error = activation_output_error + next_activation_error\n",
    "    \n",
    "    #output gate error\n",
    "    oa = lstm_activation['oa']\n",
    "    eo = np.multiply(activation_error,tanh_activation(cell_activation))\n",
    "    eo = np.multiply(np.multiply(eo,oa),1-oa)\n",
    "    \n",
    "    #cell activation error\n",
    "    cell_error = np.multiply(activation_error,oa)\n",
    "    cell_error = np.multiply(cell_error,tanh_derivative(tanh_activation(cell_activation)))\n",
    "    #error also coming from next lstm cell \n",
    "    cell_error += next_cell_error\n",
    "    \n",
    "    #input gate error\n",
    "    ia = lstm_activation['ia']\n",
    "    ga = lstm_activation['ga']\n",
    "    ei = np.multiply(cell_error,ga)\n",
    "    ei = np.multiply(np.multiply(ei,ia),1-ia)\n",
    "    \n",
    "    #gate gate error\n",
    "    eg = np.multiply(cell_error,ia)\n",
    "    eg = np.multiply(eg,tanh_derivative(ga))\n",
    "    \n",
    "    #forget gate error\n",
    "    fa = lstm_activation['fa']\n",
    "    ef = np.multiply(cell_error,prev_cell_activation)\n",
    "    ef = np.multiply(np.multiply(ef,fa),1-fa)\n",
    "    \n",
    "    #prev cell error\n",
    "    prev_cell_error = np.multiply(cell_error,fa)\n",
    "    \n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ggw = parameters['ggw']\n",
    "    ogw = parameters['ogw']\n",
    "    \n",
    "    #embedding + hidden activation error\n",
    "    embed_activation_error = np.matmul(ef,fgw.T)\n",
    "    embed_activation_error += np.matmul(ei,igw.T)\n",
    "    embed_activation_error += np.matmul(eo,ogw.T)\n",
    "    embed_activation_error += np.matmul(eg,ggw.T)\n",
    "    \n",
    "    input_hidden_units = fgw.shape[0]\n",
    "    hidden_units = fgw.shape[1]\n",
    "    input_units = input_hidden_units - hidden_units\n",
    "    \n",
    "    #prev activation error\n",
    "    prev_activation_error = embed_activation_error[:,input_units:]\n",
    "    \n",
    "    #input error (embedding error)\n",
    "    embed_error = embed_activation_error[:,:input_units]\n",
    "    \n",
    "    #store lstm error\n",
    "    lstm_error = dict()\n",
    "    lstm_error['ef'] = ef\n",
    "    lstm_error['ei'] = ei\n",
    "    lstm_error['eo'] = eo\n",
    "    lstm_error['eg'] = eg\n",
    "    \n",
    "    return prev_activation_error,prev_cell_error,embed_error,lstm_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Output Cell Derivatives for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate output cell derivatives\n",
    "def calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters):\n",
    "    #to store the sum of derivatives from each time step\n",
    "    dhow = np.zeros(parameters['how'].shape)\n",
    "    \n",
    "    batch_size = activation_cache['a1'].shape[0]\n",
    "    \n",
    "    #loop through the time steps \n",
    "    for i in range(1,len(output_error_cache)+1):\n",
    "        #get output error\n",
    "        output_error = output_error_cache['eo' + str(i)]\n",
    "        \n",
    "        #get input activation\n",
    "        activation = activation_cache['a'+str(i)]\n",
    "        \n",
    "        #cal derivative and summing up!\n",
    "        dhow += np.matmul(activation.T,output_error)/batch_size\n",
    "        \n",
    "    return dhow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate LSTM CELL Derivatives for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate derivatives for single lstm cell\n",
    "def calculate_single_lstm_cell_derivatives(lstm_error,embedding_matrix,activation_matrix):\n",
    "    #get error for single time step\n",
    "    ef = lstm_error['ef']\n",
    "    ei = lstm_error['ei']\n",
    "    eo = lstm_error['eo']\n",
    "    eg = lstm_error['eg']\n",
    "    \n",
    "    #get input activations for this time step\n",
    "    concat_matrix = np.concatenate((embedding_matrix,activation_matrix),axis=1)\n",
    "    \n",
    "    batch_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    #cal derivatives for this time step\n",
    "    dfgw = np.matmul(concat_matrix.T,ef)/batch_size\n",
    "    digw = np.matmul(concat_matrix.T,ei)/batch_size\n",
    "    dogw = np.matmul(concat_matrix.T,eo)/batch_size\n",
    "    dggw = np.matmul(concat_matrix.T,eg)/batch_size\n",
    "    \n",
    "    #store the derivatives for this time step in dict\n",
    "    derivatives = dict()\n",
    "    derivatives['dfgw'] = dfgw\n",
    "    derivatives['digw'] = digw\n",
    "    derivatives['dogw'] = dogw\n",
    "    derivatives['dggw'] = dggw\n",
    "    \n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "\n",
    "* Apply chain rule and calculate the errors for each time step\n",
    "* Store the deivatives in **derivatives** dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backpropagation\n",
    "def backward_propagation(batch_labels,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters):\n",
    "    #calculate output errors \n",
    "    output_error_cache,activation_error_cache = calculate_output_cell_error(batch_labels,output_cache,parameters)\n",
    "    \n",
    "    #to store lstm error for each time step\n",
    "    lstm_error_cache = dict()\n",
    "    \n",
    "    #to store embeding errors for each time step\n",
    "    embedding_error_cache = dict()\n",
    "    \n",
    "    # next activation error \n",
    "    # next cell error  \n",
    "    #for last cell will be zero\n",
    "    eat = np.zeros(activation_error_cache['ea1'].shape)\n",
    "    ect = np.zeros(activation_error_cache['ea1'].shape)\n",
    "    \n",
    "    #calculate all lstm cell errors (going from last time-step to the first time step)\n",
    "    for i in range(len(lstm_cache),0,-1):\n",
    "        #calculate the lstm errors for this time step 't'\n",
    "        pae,pce,ee,le = calculate_single_lstm_cell_error(activation_error_cache['ea'+str(i)],eat,ect,parameters,lstm_cache['lstm'+str(i)],cell_cache['c'+str(i)],cell_cache['c'+str(i-1)])\n",
    "        \n",
    "        #store the lstm error in dict\n",
    "        lstm_error_cache['elstm'+str(i)] = le\n",
    "        \n",
    "        #store the embedding error in dict\n",
    "        embedding_error_cache['eemb'+str(i-1)] = ee\n",
    "        \n",
    "        #update the next activation error and next cell error for previous cell\n",
    "        eat = pae\n",
    "        ect = pce\n",
    "    \n",
    "    \n",
    "    #calculate output cell derivatives\n",
    "    derivatives = dict()\n",
    "    derivatives['dhow'] = calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters)\n",
    "    \n",
    "    #calculate lstm cell derivatives for each time step and store in lstm_derivatives dict\n",
    "    lstm_derivatives = dict()\n",
    "    for i in range(1,len(lstm_error_cache)+1):\n",
    "        lstm_derivatives['dlstm'+str(i)] = calculate_single_lstm_cell_derivatives(lstm_error_cache['elstm'+str(i)],embedding_cache['emb'+str(i-1)],activation_cache['a'+str(i-1)])\n",
    "    \n",
    "    #initialize the derivatives to zeros \n",
    "    derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)\n",
    "    derivatives['digw'] = np.zeros(parameters['igw'].shape)\n",
    "    derivatives['dogw'] = np.zeros(parameters['ogw'].shape)\n",
    "    derivatives['dggw'] = np.zeros(parameters['ggw'].shape)\n",
    "    \n",
    "    #sum up the derivatives for each time step\n",
    "    for i in range(1,len(lstm_error_cache)+1):\n",
    "        derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']\n",
    "        derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']\n",
    "        derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']\n",
    "        derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']\n",
    "    \n",
    "    return derivatives,embedding_error_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer\n",
    "Using Exponentially Weighted Averages <br>\n",
    "* Vdw = beta1 x Vdw + (1-beta1) x (dw)   \n",
    "* Sdw = beta2 x Sdw + (1-beta2) x dw^2\n",
    "* W = W - learning_rate x ( Vdw/ (sqrt(Sdw)+1e-7) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the parameters using adam optimizer\n",
    "#adam optimization\n",
    "def update_parameters(parameters,derivatives,V,S,t):\n",
    "    #get derivatives\n",
    "    dfgw = derivatives['dfgw']\n",
    "    digw = derivatives['digw']\n",
    "    dogw = derivatives['dogw']\n",
    "    dggw = derivatives['dggw']\n",
    "    dhow = derivatives['dhow']\n",
    "    \n",
    "    #get parameters\n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    how = parameters['how']\n",
    "    \n",
    "    #get V parameters\n",
    "    vfgw = V['vfgw']\n",
    "    vigw = V['vigw']\n",
    "    vogw = V['vogw']\n",
    "    vggw = V['vggw']\n",
    "    vhow = V['vhow']\n",
    "    \n",
    "    #get S parameters\n",
    "    sfgw = S['sfgw']\n",
    "    sigw = S['sigw']\n",
    "    sogw = S['sogw']\n",
    "    sggw = S['sggw']\n",
    "    show = S['show']\n",
    "    \n",
    "    #calculate the V parameters from V and current derivatives\n",
    "    vfgw = (beta1*vfgw + (1-beta1)*dfgw)\n",
    "    vigw = (beta1*vigw + (1-beta1)*digw)\n",
    "    vogw = (beta1*vogw + (1-beta1)*dogw)\n",
    "    vggw = (beta1*vggw + (1-beta1)*dggw)\n",
    "    vhow = (beta1*vhow + (1-beta1)*dhow)\n",
    "    \n",
    "    #calculate the S parameters from S and current derivatives\n",
    "    sfgw = (beta2*sfgw + (1-beta2)*(dfgw**2))\n",
    "    sigw = (beta2*sigw + (1-beta2)*(digw**2))\n",
    "    sogw = (beta2*sogw + (1-beta2)*(dogw**2))\n",
    "    sggw = (beta2*sggw + (1-beta2)*(dggw**2))\n",
    "    show = (beta2*show + (1-beta2)*(dhow**2))\n",
    "    \n",
    "    #update the parameters\n",
    "    fgw = fgw - learning_rate*((vfgw)/(np.sqrt(sfgw) + 1e-6))\n",
    "    igw = igw - learning_rate*((vigw)/(np.sqrt(sigw) + 1e-6))\n",
    "    ogw = ogw - learning_rate*((vogw)/(np.sqrt(sogw) + 1e-6))\n",
    "    ggw = ggw - learning_rate*((vggw)/(np.sqrt(sggw) + 1e-6))\n",
    "    how = how - learning_rate*((vhow)/(np.sqrt(show) + 1e-6))\n",
    "    \n",
    "    #store the new weights\n",
    "    parameters['fgw'] = fgw\n",
    "    parameters['igw'] = igw\n",
    "    parameters['ogw'] = ogw\n",
    "    parameters['ggw'] = ggw\n",
    "    parameters['how'] = how\n",
    "    \n",
    "    #store the new V parameters\n",
    "    V['vfgw'] = vfgw \n",
    "    V['vigw'] = vigw \n",
    "    V['vogw'] = vogw \n",
    "    V['vggw'] = vggw\n",
    "    V['vhow'] = vhow\n",
    "    \n",
    "    #store the s parameters\n",
    "    S['sfgw'] = sfgw \n",
    "    S['sigw'] = sigw \n",
    "    S['sogw'] = sogw \n",
    "    S['sggw'] = sggw\n",
    "    S['show'] = show\n",
    "    \n",
    "    return parameters,V,S    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the Embeddings\n",
    "def update_embeddings(embeddings,embedding_error_cache,batch_labels):\n",
    "    #to store the embeddings derivatives\n",
    "    embedding_derivatives = np.zeros(embeddings.shape)\n",
    "    \n",
    "    batch_size = batch_labels[0].shape[0]\n",
    "    \n",
    "    #sum the embedding derivatives for each time step\n",
    "    for i in range(len(embedding_error_cache)):\n",
    "        embedding_derivatives += np.matmul(batch_labels[i].T,embedding_error_cache['eemb'+str(i)])/batch_size\n",
    "    \n",
    "    #update the embeddings\n",
    "    embeddings = embeddings - learning_rate*embedding_derivatives\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Initialize the V and S parameters for Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_V(parameters):\n",
    "    Vfgw = np.zeros(parameters['fgw'].shape)\n",
    "    Vigw = np.zeros(parameters['igw'].shape)\n",
    "    Vogw = np.zeros(parameters['ogw'].shape)\n",
    "    Vggw = np.zeros(parameters['ggw'].shape)\n",
    "    Vhow = np.zeros(parameters['how'].shape)\n",
    "    \n",
    "    V = dict()\n",
    "    V['vfgw'] = Vfgw\n",
    "    V['vigw'] = Vigw\n",
    "    V['vogw'] = Vogw\n",
    "    V['vggw'] = Vggw\n",
    "    V['vhow'] = Vhow\n",
    "    return V\n",
    "\n",
    "def initialize_S(parameters):\n",
    "    Sfgw = np.zeros(parameters['fgw'].shape)\n",
    "    Sigw = np.zeros(parameters['igw'].shape)\n",
    "    Sogw = np.zeros(parameters['ogw'].shape)\n",
    "    Sggw = np.zeros(parameters['ggw'].shape)\n",
    "    Show = np.zeros(parameters['how'].shape)\n",
    "    \n",
    "    S = dict()\n",
    "    S['sfgw'] = Sfgw\n",
    "    S['sigw'] = Sigw\n",
    "    S['sogw'] = Sogw\n",
    "    S['sggw'] = Sggw\n",
    "    S['show'] = Show\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "\n",
    "1. Initialize Parameters\n",
    "2. Forward Propagation\n",
    "3. Calculate Loss, Perplexity and Accuracy\n",
    "4. Backward Propagation\n",
    "5. Update the Parameters and Embeddings\n",
    "\n",
    "Batch Size = 20\n",
    "Repeat the steps 2-5 for each batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(train_dataset,iters=1000,batch_size=20):\n",
    "    #initalize the parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    #initialize the V and S parameters for Adam\n",
    "    V = initialize_V(parameters)\n",
    "    S = initialize_S(parameters)\n",
    "    \n",
    "    #generate the random embeddings\n",
    "    embeddings = np.random.normal(0,0.01,(len(vocab),input_units))\n",
    "    \n",
    "    #to store the Loss, Perplexity and Accuracy for each batch\n",
    "    J = []\n",
    "    P = []\n",
    "    A = []\n",
    "    \n",
    "    \n",
    "    for step in range(iters):\n",
    "        #get batch dataset\n",
    "        index = step%len(train_dataset)\n",
    "        batches = train_dataset[index]\n",
    "        \n",
    "        #forward propagation\n",
    "        embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache = forward_propagation(batches,parameters,embeddings)\n",
    "        \n",
    "        #calculate the loss, perplexity and accuracy\n",
    "        perplexity,loss,acc = cal_loss_accuracy(batches,output_cache)\n",
    "        \n",
    "        #backward propagation\n",
    "        derivatives,embedding_error_cache = backward_propagation(batches,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters)\n",
    "        \n",
    "        #update the parameters\n",
    "        parameters,V,S = update_parameters(parameters,derivatives,V,S,step)\n",
    "        \n",
    "        #update the embeddings\n",
    "        embeddings = update_embeddings(embeddings,embedding_error_cache,batches)\n",
    "        \n",
    "        \n",
    "        J.append(loss)\n",
    "        P.append(perplexity)\n",
    "        A.append(acc)\n",
    "        \n",
    "        #print loss, accuracy and perplexity\n",
    "        if(step%1000==0):\n",
    "            print(\"For Single Batch :\")\n",
    "            print('Step       = {}'.format(step))\n",
    "            print('Loss       = {}'.format(round(loss,2)))\n",
    "            print('Perplexity = {}'.format(round(perplexity,2)))\n",
    "            print('Accuracy   = {}'.format(round(acc*100,2)))\n",
    "            print()\n",
    "    \n",
    "    return embeddings, parameters,J,P,A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Train\n",
    "* Will take around 5-10 mins on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_units = 1004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings,parameters,J,P,A = train(flattened,iters=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = list()\n",
    "avg_acc = list()\n",
    "avg_perp = list()\n",
    "i = 0\n",
    "while(i<len(J)):\n",
    "    avg_loss.append(np.mean(J[i:i+30]))\n",
    "    avg_acc.append(np.mean(A[i:i+30]))\n",
    "    avg_perp.append(np.mean(P[i:i+30]))\n",
    "    i += 30\n",
    "\n",
    "plt.plot(list(range(len(avg_loss))),avg_loss)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Loss (Avg of 30 batches)\")\n",
    "plt.title(\"Loss Graph\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(len(avg_perp))),avg_perp)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Perplexity (Avg of 30 batches)\")\n",
    "plt.title(\"Perplexity Graph\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(len(avg_acc))),avg_acc)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Accuracy (Avg of 30 batches)\")\n",
    "plt.title(\"Accuracy Graph\")\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
